Updated git hooks.
Git LFS initialized.
fatal: destination path 'opus-mt-de-en' already exists and is not an empty directory.
2022-03-14 23:17:00.578274: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-03-14 23:17:00.578352: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
mkdir: cannot create directory ‘my-tokenizer’: File exists
sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=train/train.my --model_prefix=my-tokenizer/spm --vocab_size=50000 --character_coverage=1.0 --hard_vocab_limit=false
sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : 
trainer_spec {
  input: train/train.my
  input_format: 
  model_prefix: my-tokenizer/spm
  model_type: UNIGRAM
  vocab_size: 50000
  self_test_sample_size: 0
  character_coverage: 1
  input_sentence_size: 0
  shuffle_input_sentence: 1
  seed_sentencepiece_size: 1000000
  shrinking_factor: 0.75
  max_sentence_length: 4192
  num_threads: 16
  num_sub_iterations: 2
  max_sentencepiece_length: 16
  split_by_unicode_script: 1
  split_by_number: 1
  split_by_whitespace: 1
  split_digits: 0
  treat_whitespace_as_suffix: 0
  allow_whitespace_only_pieces: 0
  required_chars: 
  byte_fallback: 0
  vocabulary_output_piece_score: 1
  train_extremely_large_corpus: 0
  hard_vocab_limit: 0
  use_all_vocab: 0
  unk_id: 0
  bos_id: 1
  eos_id: 2
  pad_id: -1
  unk_piece: <unk>
  bos_piece: <s>
  eos_piece: </s>
  pad_piece: <pad>
  unk_surface:  ⁇ 
}
normalizer_spec {
  name: nmt_nfkc
  add_dummy_prefix: 1
  remove_extra_whitespaces: 1
  escape_whitespaces: 1
  normalization_rule_tsv: 
}
denormalizer_spec {}
trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.
trainer_interface.cc(178) LOG(INFO) Loading corpus: train/train.my
trainer_interface.cc(356) LOG(WARNING) Found too long line (4419 > 4192).
trainer_interface.cc(358) LOG(WARNING) Too long lines are skipped in the training.
trainer_interface.cc(359) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.
trainer_interface.cc(385) LOG(INFO) Loaded all 876904 sentences
trainer_interface.cc(391) LOG(INFO) Skipped 1080 too long sentences.
trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>
trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>
trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>
trainer_interface.cc(405) LOG(INFO) Normalizing sentences...
trainer_interface.cc(466) LOG(INFO) all chars count=93426324
trainer_interface.cc(477) LOG(INFO) Done: 100% characters are covered.
trainer_interface.cc(487) LOG(INFO) Alphabet size=3179
trainer_interface.cc(488) LOG(INFO) Final character coverage=1
trainer_interface.cc(520) LOG(INFO) Done! preprocessed 876286 sentences.
unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...
unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...
unigram_model_trainer.cc(194) LOG(INFO) Initialized 1000000 seed sentencepieces
trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 876286
trainer_interface.cc(537) LOG(INFO) Done! 2439564
unigram_model_trainer.cc(489) LOG(INFO) Using 2439564 sentences for EM training
unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=484060 obj=25.0097 num_tokens=8534008 num_tokens/piece=17.6301
unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=327867 obj=20.2841 num_tokens=8584425 num_tokens/piece=26.1826
unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=245033 obj=20.0819 num_tokens=8658931 num_tokens/piece=35.3378
unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=240007 obj=20.0065 num_tokens=8684054 num_tokens/piece=36.1825
unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=179889 obj=20.0331 num_tokens=8775886 num_tokens/piece=48.785
unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=179757 obj=20.0056 num_tokens=8783721 num_tokens/piece=48.8644
unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=134794 obj=20.1172 num_tokens=8943613 num_tokens/piece=66.3502
unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=134754 obj=20.0764 num_tokens=8947331 num_tokens/piece=66.3975
unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=101058 obj=20.2942 num_tokens=9170415 num_tokens/piece=90.7441
unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=101045 obj=20.222 num_tokens=9172776 num_tokens/piece=90.7791
unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=75782 obj=20.5245 num_tokens=9428479 num_tokens/piece=124.416
unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=75779 obj=20.4356 num_tokens=9429612 num_tokens/piece=124.436
unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=56833 obj=20.8126 num_tokens=9716959 num_tokens/piece=170.974
unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=56832 obj=20.7125 num_tokens=9718274 num_tokens/piece=171
unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=54999 obj=20.7527 num_tokens=9752324 num_tokens/piece=177.318
unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=54999 obj=20.7407 num_tokens=9754022 num_tokens/piece=177.349
trainer_interface.cc(615) LOG(INFO) Saving model: my-tokenizer/spm.model
trainer_interface.cc(626) LOG(INFO) Saving vocabs: my-tokenizer/spm.vocab
train/train.my
Traceback (most recent call last):
  File "/root/transfer-mt-submit-software/predownload.py", line 28, in <module>
    spm.SentencePieceTrainer.train(f'--input={source_input_path} --model_prefix={lang}-tokenizer/spm --vocab_size=50000 --character_coverage=1.0 --hard_vocab_limit=false')
  File "/root/miniconda3/lib/python3.9/site-packages/sentencepiece/__init__.py", line 407, in Train
    return SentencePieceTrainer._TrainFromString(arg)
  File "/root/miniconda3/lib/python3.9/site-packages/sentencepiece/__init__.py", line 385, in _TrainFromString
    return _sentencepiece.SentencePieceTrainer__TrainFromString(arg)
KeyboardInterrupt
